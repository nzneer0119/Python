{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93586fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62eb1982",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f6879",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
    "- 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\n",
    "- 날짜,내용요약본  -> 정제 작업 필요\n",
    "- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장 \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "#각 크롤링 결과 저장하기 위한 리스트 선언 \n",
    "title_text=[]\n",
    "link_text=[]\n",
    "source_text=[]\n",
    "date_text=[]\n",
    "contents_text=[]\n",
    "result={}\n",
    "\n",
    "#엑셀로 저장하기 위한 변수\n",
    "RESULT_PATH ='~/Desktop/'  #결과 저장할 경로 (바탕화면에 저장됨)\n",
    "\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기\n",
    "\n",
    "#날짜 정제화 함수\n",
    "def date_cleansing(test):\n",
    "    try:\n",
    "        #지난 뉴스\n",
    "        #머니투데이  10면1단  2018.11.05.  네이버뉴스   보내기  \n",
    "        pattern = '\\d+.(\\d+).(\\d+).'  #정규표현식 \n",
    "    \n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0)  # 2018.11.05.\n",
    "        date_text.append(match)\n",
    "        #print(match)\n",
    "        \n",
    "    except AttributeError:\n",
    "        #최근 뉴스\n",
    "        #이데일리  1시간 전  네이버뉴스   보내기  \n",
    "\n",
    "        pattern = '(\\d\\w+\\s\\전)' #정규표현식 \n",
    "\n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0)\n",
    "        #print(match)\n",
    "        date_text.append(match)\n",
    "\n",
    "#내용 정제화 함수 \n",
    "\n",
    "def contents_cleansing(contents):\n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', \n",
    "                                      str(contents)).strip()  #앞에 필요없는 부분 제거\n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', \n",
    "                                       first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "    contents_text.append(third_cleansing_contents)\n",
    "    #print(contents_text)\n",
    "    \n",
    "\n",
    "def crawler(maxpage,query,sort,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1  \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    \n",
    "    while page <= maxpage_t:\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        # url 주소에다 날짜를 입력할 예정\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "\n",
    "        #뷰티풀소프의 인자값 지정\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #print(soup)\n",
    "        #<a>태그에서 제목과 링크주소 추출\n",
    "        \n",
    "        atags = soup.select('.news_tit') # 뉴스기사의 제목 셀렉트\n",
    "        \n",
    "        for atag in atags:\n",
    "            title_text.append(atag.text)     #제목 따오기\n",
    "            link_text.append(atag['href'])   #링크주소 따오기\n",
    "        \n",
    "        #신문사 추출\n",
    "        \n",
    "        source_lists = soup.select('.press')\n",
    "        \n",
    "        for source_list in source_lists:\n",
    "            if source_list.i:\n",
    "                source_list.i.extract()\n",
    "            if source_list.find('span') == None:\n",
    "                continue\n",
    "            source_text.append(source_list.text)    #신문사\n",
    "            \n",
    "        #날짜 추출 \n",
    "        \n",
    "        date_lists = soup.select('.info_group')\n",
    "        #print(date_lists)\n",
    "        \n",
    "        for date_list in date_lists:\n",
    "            if date_list.i:\n",
    "                date_list.i.extract()\n",
    "            test=date_list.text   \n",
    "            \n",
    "            date_cleansing(test)  #날짜 정제 함수사용 \n",
    "            \n",
    "\n",
    "        #본문요약본\n",
    "        contents_lists = soup.select('.dsc_txt_wrap')\n",
    "        for contents_list in contents_lists:\n",
    "            #print('==='*40)\n",
    "            #print(contents_list)\n",
    "            contents_cleansing(contents_list) #본문요약 정제화\n",
    "        \n",
    "        \n",
    "        #모든 리스트 딕셔너리형태로 저장\n",
    "        \n",
    "        # print(len(date_text), len(title_text), len(source_text), len(contents_text), len(link_text))\n",
    "        result= {\"date\" : date_text , \"title\":title_text ,  \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }  \n",
    "        \n",
    "        #print(page)\n",
    "        \n",
    "        #print(result)\n",
    "        df = pd.DataFrame(result)  #df로 변환\n",
    "        page += 10\n",
    "        #df\n",
    "    \n",
    "    \n",
    "    # 새로 만들 파일이름 지정\n",
    "    #outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    \n",
    "    outputFileName = query + ' %s-%s-%s  %s시 %s분.xlsx' % (now.year, now.month, now.day, now.hour, now.minute)\n",
    "    \n",
    "    df.to_excel(RESULT_PATH+outputFileName,sheet_name='sheet1')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    query = input(\"검색어 입력: \")  \n",
    "    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")  \n",
    "    \n",
    "    sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \") \n",
    "    s_date = input(\"시작날짜 입력(2020.02.18):\") \n",
    "    e_date = input(\"끝날짜 입력(2020.02.18):\")   \n",
    "    \n",
    "    \n",
    "    crawler(maxpage,query,sort,s_date,e_date)\n",
    "    \n",
    "    print('크롤링 중입니다.')\n",
    "    time.sleep(2) # 10초\n",
    "    print('저장이 완료되었습니다.')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9992124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주세요 :야식추천\n",
      "조회 빈도 상위 몇위 까지 확인 할까요? 10\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from nltk import Text\n",
    "from konlpy.tag import *        #pip install konlpy 먼저 하세요\n",
    "import matplotlib.pyplot as plt #pip install matplotlib 먼저 하세요\n",
    "from matplotlib import font_manager , rc\n",
    "from wordcloud import WordCloud  # pip install wordcloud 먼저 하세요\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "text = input(\"검색어를 입력해주세요 :\") # 검색어\n",
    "# text1 = int(input(\"시작페이지 설정: \"))\n",
    "# text2 = int(input(\"마지막페이지 설정:\"))\n",
    "text1 = 1\n",
    "text2 = 570\n",
    "text3 = int(input(\"조회 빈도 상위 몇위 까지 확인 할까요? \"))\n",
    "\n",
    "path = \"D:/00_Limhs/py_data/drv/chromedriver.exe\" # 웹드라이버 실행\n",
    " \n",
    "RESULT_PATH ='~/Desktop/' \n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(path) # 드라이버 경로 설정\n",
    "url_list = [] # 블로그 url을 저장하기 위한 변수\n",
    "content_list = \"\" # 블로그 content를 누적하기 위한 변수\n",
    "\n",
    "\n",
    " \n",
    "for i in range((text1),(text2)):  # 1~100페이지까지의 블로그 내용을 읽어옴\n",
    "    url = 'https://section.blog.naver.com/Search/Post.nhn?pageNo='+ str(i) + '&rangeType=ALL&orderBy=sim&keyword=' + text # url 값 설정\n",
    "    driver.get(url)\n",
    "    time.sleep(0.5) # 오류 방지 sleep\n",
    "\n",
    "    for j in range(1, 3):\n",
    "        titles = driver.find_element_by_xpath('/html/body/ui-view/div/main/div/div/section/div[2]/div['+str(j)+']/div/div[1]/div[1]/a[1]')\n",
    "        title = titles.get_attribute('href')\n",
    "        url_list.append(title)\n",
    "\n",
    "print(\"url 수집 끝, 해당 url 데이터 크롤링\")\n",
    " \n",
    "for url in url_list: # 저장했던 블로그 하나씩 순회\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    overlays = \".se-component.se-text.se-l-default\" # 내용 크롤링\n",
    "    contents = driver.find_elements_by_css_selector(overlays)\n",
    "     \n",
    "    for content in contents:\n",
    "        content_list = content_list + content.text # 각 블로그의 내용을 변수에 누적함\n",
    "\n",
    "# 트위터에서 만든 소셜 분석을 위한 형태소 분석기 Okt 사용\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma( )\n",
    "myList = okt.pos(content_list, norm=True, stem=True) # 모든 형태소 추출\n",
    "myList_filter = [x for x, y in myList if y in ['Noun']] # 추출된 값 중 동사만 추출\n",
    "  \n",
    "Okt = Text(myList_filter, name=\"Okt\")\n",
    "\n",
    "#okt.pos(data1)\n",
    "\n",
    "# 그래프에서 한글이 출력이 안되는 문제 해결 (ㅁㅁㅁ 처럼 출력됨)\n",
    "\n",
    "font_location = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "\n",
    "rc('font', family=font_name)\n",
    " \n",
    "    \n",
    "# 그래프 x, y 라벨 설정\n",
    "plt.xlabel(\"key words\")\n",
    "plt.ylabel(\"frequency\")\n",
    " \n",
    "# 그래프에서 x, y 값을 설정\n",
    "\n",
    "wordInfo = dict()\n",
    "for tags, counts in Okt.vocab().most_common(text3): # 출력 갯수 제한\n",
    "    \n",
    "    if(len(str(tags)) > 0):\n",
    "        wordInfo[tags] = counts\n",
    "\n",
    "    values = sorted(wordInfo.values(), reverse=True)\n",
    "    keys = sorted(wordInfo, key=wordInfo.get, reverse=True)\n",
    "\n",
    "# 그래프 값 설정\n",
    "plt.bar(range(len(wordInfo)), values, align='center')\n",
    "plt.xticks(range(len(wordInfo)), list(keys), rotation='70')\n",
    "plt.show()\n",
    " \n",
    "# wordCloud 출력\n",
    "wc = WordCloud(width = 1000, height = 600, background_color=\"white\", font_path=font_location, max_words=50)\n",
    "plt.imshow(wc.generate_from_frequencies(Okt.vocab()))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30f82095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"921cab4a-472d-48da-83bb-98d9c88ae920\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"d8b2d24a-ce03-4b36-b0ec-90096a537185\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"18e30de0-115e-424a-8f9e-d1ef70291517\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"97a3bf90-8d04-44f4-bfd6-6614e2b84d99\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"c0684ba3-8c8e-487f-a3ee-dc8327410c41\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"20f7a6e4-57c2-4c68-b06f-33909c200922\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"5e2beb17-da45-4243-b68a-0426a0d800ba\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"2c6269bb-7f42-4796-865a-82ad1b675946\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"89004f79-0b49-4b57-83b6-957a7e53e695\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"bdbaf8e6-792c-471a-a6a8-49df879ffe15\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"06c8fdb4-6b15-41c5-b30b-cea136da6128\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"9d155fb4-3f68-4b50-81b6-7ba3cca7942a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"0cc00abe-67c6-4bf3-ba37-faa76a487c01\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"9ce64990-7cd3-4d73-a482-83c69127a518\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"2c34b7de-26c3-4f9a-a3e6-dd4d4f420ebf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"ecce93cb-bb7b-4dae-beb9-bfd81a15c557\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"b1809940-c467-474d-98d0-17f32b606201\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"f735e61d-0e57-4da0-a2bb-4c388a6ab155\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"ba4653a4-9a3a-4d1e-b1af-68ab15cc0dac\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"bb0997ba-6a22-4abf-8b33-c7779b040b7b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"91dbe557-d6a4-47af-aefa-217f0b9412a6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"63b9b498c35770e92c65af32d5206da9\", element=\"3228422b-bdce-4743-aacb-461b5f6dd7c4\")>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb27bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b55c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023ca80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f4b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55129753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
